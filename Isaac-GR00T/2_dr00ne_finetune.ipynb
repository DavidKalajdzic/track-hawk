{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finetuning is done offline, from the data that we managed to load and infer previously we are trying to train the model to perform better on the dataset, meaning with the input images and the drone state, we want to predict the best drone action. Gr00t predicts in fact the next 16 timesteps of the drone action. We will take the first of those 16 as the actual action to perform...",
   "id": "350ad16f813b29a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To run the finetuning use the run_finetuning.sh script\n",
    "\n",
    "We finetuned mainly two models\n",
    "\n",
    "1) Model with llm and visual frozen and projector and diffusion model tuned. Using LoRA with rank 32\n",
    "\n",
    "python track-hawk/Isaac-GR00T/scripts/gr00t_finetune.py \\\n",
    "  --dataset-path track-hawk/data_track_hawk/dataset_drone_control/ \\\n",
    "  --output-dir track-hawk/checkpoints1 \\\n",
    "  --data-config track_hawk \\\n",
    "  --batch-size 32 \\\n",
    "  --max-steps 5000 \\\n",
    "  --num-gpus 4 \\\n",
    "  --save-steps 1000 \\\n",
    "  --base-model-path nvidia/GR00T-N1-2B \\\n",
    "  --no-tune-llm \\\n",
    "  --no-tune-visual \\\n",
    "  --tune-projector \\\n",
    "  --tune-diffusion-model \\\n",
    "  --learning-rate 1e-4 \\\n",
    "  --weight-decay 1e-5 \\\n",
    "  --warmup-ratio 0.05 \\\n",
    "  --lora-rank 32 \\\n",
    "  --lora-alpha 16 \\\n",
    "  --lora-dropout 0.1 \\\n",
    "  --dataloader-num-workers 16 \\\n",
    "  --report-to wandb \\\n",
    "  --embodiment-tag new_embodiment \\\n",
    "  --video-backend decord\n",
    "\n",
    "2) Same as Model 1 but with LoRA with rank 0, this leaded to larger and a bit better model\n",
    "\n",
    "python track-hawk/Isaac-GR00T/scripts/gr00t_finetune.py \\\n",
    "  --dataset-path track-hawk/data_track_hawk/dataset_drone_control/ \\\n",
    "  --output-dir track-hawk/checkpoints1 \\\n",
    "  --data-config track_hawk \\\n",
    "  --batch-size 32 \\\n",
    "  --max-steps 5000 \\\n",
    "  --num-gpus 4 \\\n",
    "  --save-steps 1000 \\\n",
    "  --base-model-path nvidia/GR00T-N1-2B \\\n",
    "  --no-tune-llm \\\n",
    "  --no-tune-visual \\\n",
    "  --tune-projector \\\n",
    "  --tune-diffusion-model \\\n",
    "  --learning-rate 1e-4 \\\n",
    "  --weight-decay 1e-5 \\\n",
    "  --warmup-ratio 0.05 \\\n",
    "  --lora-rank 0 \\\n",
    "  --lora-alpha 16 \\\n",
    "  --lora-dropout 0.1 \\\n",
    "  --dataloader-num-workers 16 \\\n",
    "  --report-to wandb \\\n",
    "  --embodiment-tag new_embodiment \\\n",
    "  --video-backend decord\n",
    "\n",
    "Our Model 2 is on Huggingface: https://huggingface.co/DavidKalajdzic/gr00t-drone-lora-rank0\n"
   ],
   "id": "49a3d62ed14f397c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Go to the evaluate finetuned model for graphical analysis of results",
   "id": "67a4b18ed927a9de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
